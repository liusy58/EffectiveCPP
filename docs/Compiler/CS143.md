## 14-01 Intermediate language
#### What is Intermediate language?
The intermediate language is a language between the source and the target.

#### Why bother to introduce Intermediate language?
Because it provides an intermediate level of abstraction, it has more details than the source. For example, we want to optimize register usage, most source languages have no notion of the register at the source level so there is no way to even express the kinds of optimization you might want to do with registers.
It will also have fewer details than the target. For example, the intermediate language is a little bit above the level of the particular instruction set of a particular machine, and therefore, it is easier to retarget that intermediate level of code to lots of different kinds of machines because it doesn't have all the grubby details of a particular machine.

## 14-02 Optimization Overview

#### When should we perform optimizations?

In fact, we can perform them on the AST, a big advantage of that is that it's machine-independent, but it turns out AST is too high for a lot of optimizations we want to do because optimizations depend on lower-level details of the machine. Another possibility would be to perform optimizations directly on the assembly language but they are machine-dependent and we would have to reimplement optimizations for each kind of architecture. So intermediate language is an ideal choice.

#### What is the purpose of optimization?
The purpose of optimization is to improve a program's resource utilization such as execution time, code size, network messages sent, and so on.
For languages like C and Cool,  there are three granularities of optimizations. One is called Local optimizations which occur within a single block. Then there're what are called Global optimizations. This is really misnamed because it is not global across the entire program, it's global across an entire function. So global optimizations would apply to a single function and apply to all the basic blocks of that function. And finally, there are inter-procedural optimizations. These are optimizations that work across method boundaries. They take multiple functions and move things around to try to optimize the collection of functions as a whole. 

#### In practice, often a conscious decision is made not to implement the fanciest optimization known, why?
First, some optimizations are hard to implement for SE. 
Second, some optimizations are costly in compilation time. Even though the compiling happens offline not part of the running of the program, the programmer still has to wait when the optimizing compiler does its optimizations. 
Third, some of these optimizations have a low payoff. They might only do it by a small amount.
Last, unfortunately, many fancy optimizations are all there!

So the goal of optimization is to maximum benefit for minimum cost.

## 14-03 Local Optimizations

### Algebraic Simplification

#### Some statements can  be deleted.
```C
x := x + 0; 
x := x * 1;
```
#### Some statements can be simplified.
```C
x := x * 0;     =>  x := 0;
y := y ** 2;    =>  y := y * y;
z := z * 8;     =>  z := z << 3;
p := p * 15;    =>  t := p << 4; p : = t - p;
```

For the operator `**`, it's probably that's going to wind up in our generated code is a call to some built-in math library which will introduce the function call overhead and some kind of general loop in there. So in a special case where we know the exponent is 2, it's much more efficient to just replace that call to exponentiate by an explicit multiply. 

Another example is `z := z * 8;`. If we have a multiplication by a power of `2`, we can replace that with a left bit shift. In fact, it doesn't have to be a power of two if we have a multiplication by some other number, that can be replaced by some combination of shifting and subtractions. I want to point out that **these transformations will not result in any kind of speedup** because, on modern machines, multiplication is just as fast as any other single instruction. 
 
### constant folding
Operations on constants can be computed at compile time. 
```C
x := 2 + 2;  => x := 4;
if  2 > 0 jump L  => jump L;
```

There is one situation that you should be aware of in which constant folding can be very dangerous. It's something that really illustrates some of the subtleties of program optimization and programming language semantics. Let's consider the scenario where we have two machines, X, Y.   Now the compiler is running on machine X, and the compiler is producing code for machine Y, and the code will run on it. This is called cross-compiler(Just considering the embedded system code). The problem comes, if X and Y are different architectures. So let's say we have the instruction `a := 1.5 + 3.7;`, and you would like to constant fold that down to equal `5.2`. Now the problem is that if you simply execute this as a floating-point operation on X, the roundoff and the pointing number semantics may be slightly different from Y. So on Y you may get something like `a:= 5.19;` . There might be a small difference in the floating-point result. 


### Eliminate unreachable basic blocks
An unreachable basic block is one that is not the target of any jump or falls through. It will make the code smaller and run faster because of the cache effects to increase the spatial locality.

#### Why would unreachable basic blocks occur?
There are several ways in which unreachable code can arise.  The most common cause is that the code is actually parameterized with code that is only compiled and used in certain situations. In c, it would be sort of typical to see some code that looks like this:

```C
#define DEBUG 0

if(DEBUG){ ...}
```

Another case where unreachable code comes up is with libraries. So very frequently,  programs are written to use generic libraries that the program might only use a very small part of the interface. But the library might supply hundreds of methods to cover all the situations. But for your program, you might only be using three of those methods and the rest of the methods could potentially be removed from the final binary to make the code smaller. 

And finally,  another way that unreachable basic blocks occur is as the result of other optimizations. So as we can see, optimizations frequently lead to more optimizations. And it could just be through other rearrangements of the compiler to make some redundant BBs removed. 

### Some optimizations are simplified if each register occurs only once on the left-hand side of an assignment.

If each register is assigned at most once then some of these optimizations are easier to talk about.

#### Common subexpression elimination

In SSA and `x := ` is the first use of `x` in a block. Then when two assignments have the same rhs, they compute the same value.

```C
x := y + z;
...
w := y + z;
```
can be replaced by :

```C
x := y + z;
...
w := x;
```

#### copy propogation

If `w := x `appears in a block, replace subsequent uses of `w` with the use of `x`.

```C
b := z + y;
a := b;
x := 2 * a;
```

can be replaced by:

```C
b := z + y;
a := b;
x := 2 * b;
```

This optimization can only be useful in conjunction with some of other optimizations such as constant folding and dead code elimination.

### Sumarry

Each of these optimizations presented actually doesn't make the program run faster at all. They don't make program run slower either, but by themselves, they don't actually make any improvement to the program. But typically, the optimizations will interact , so performing one optimization will enable another. So the way to think about an optimizing compiler is that it has a big bag of tricks(program transformations), when faced with a program to optimize,  it is going to rummage around in its bag looking for an optimization that applies to some part of the code. If it finds one,  it will do the optimization. And it will repeat it and look back and see if there is another optimization that applies. Then it will just keep doing this until it reaches a point where none of the optimizations it knows can be applied. 

### 14-04 Peephole Optimizations

Let's see a variation on local optimization that applies directly to assembly code called peephole optimization. Peephole optimization in one such technique that peephole stands for a short (usually continuous) sequence of instructions, what optimizer will do is to replace the sequence with another equivalent one(but faster). 

One example is:

```asm
move $a $b
move $b $a
```

can be replaced by the following code if `move $b $a` is not the target of a jump.

```asm
move $a $b
```

Another example is

```asm
addiu $a $a i;
addiu $a $a j;
```
can be replaced by the following code

```asm
addiu $a $a i+j;
```

Many basic block optimizations can be cast also as peephole optimizations.:

* `addiu $a $b 0` -> `move $a $b`
* `move $a $a` -> 

Peephole optimizations must be applied repeatedly for maximum effect. In fact, program optimization is grossly misnamed, I think program improvement is a more appropriate term because compilers will just improve the program as much as they can, and there is no guarantee that the best code will be produced. 
