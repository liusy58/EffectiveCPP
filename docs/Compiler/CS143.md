## 14-01 Intermediate language
#### What is Intermediate language?
The intermediate language is a language between the source and the target.

#### Why bother to introduce Intermediate language?
Because it provides an intermediate level of abstraction, it has more details than the source. For example, we want to optimize register usage, most source languages have no notion of the register at the source level so there is no way to even express the kinds of optimization you might want to do with registers.
It will also have fewer details than the target. For example, the intermediate language is a little bit above the level of the particular instruction set of a particular machine, and therefore, it is easier to retarget that intermediate level of code to lots of different kinds of machines because it doesn't have all the grubby details of a particular machine.

## 14-02 Optimization Overview

#### When should we perform optimizations?

In fact, we can perform them on the AST, a big advantage of that is that it's machine-independent, but it turns out AST is too high for a lot of optimizations we want to do because optimizations depend on lower-level details of the machine. Another possibility would be to perform optimizations directly on the assembly language but they are machine-dependent and we would have to reimplement optimizations for each kind of architecture. So intermediate language is an ideal choice.

#### What is the purpose of optimization?
The purpose of optimization is to improve a program's resource utilization such as execution time, code size, network messages sent, and so on.
For languages like C and Cool,  there are three granularities of optimizations. One is called Local optimizations which occur within a single block. Then there're what are called Global optimizations. This is really misnamed because it is not global across the entire program, it's global across an entire function. So global optimizations would apply to a single function and apply to all the basic blocks of that function. And finally, there are inter-procedural optimizations. These are optimizations that work across method boundaries. They take multiple functions and move things around to try to optimize the collection of functions as a whole. 

#### In practice, often a conscious decision is made not to implement the fanciest optimization known, why?
First, some optimizations are hard to implement for SE. 
Second, some optimizations are costly in compilation time. Even though the compiling happens offline not part of the running of the program, the programmer still has to wait when the optimizing compiler does its optimizations. 
Third, some of these optimizations have a low payoff. They might only do it by a small amount.
Last, unfortunately, many fancy optimizations are all there!

So the goal of optimization is to maximum benefit for minimum cost.

## 14-03 Local Optimizations

### Algebraic Simplification

#### Some statements can  be deleted.
```C
x := x + 0; 
x := x * 1;
```
#### Some statements can be simplified.
```C
x := x * 0;     =>  x := 0;
y := y ** 2;    =>  y := y * y;
z := z * 8;     =>  z := z << 3;
p := p * 15;    =>  t := p << 4; p : = t - p;
```

For the operator `**`, it's probably that's going to wind up in our generated code is a call to some built-in math library which will introduce the function call overhead and some kind of general loop in there. So in a special case where we know the exponent is 2, it's much more efficient to just replace that call to exponentiate by an explicit multiply. 

Another example is `z := z * 8;`. If we have a multiplication by a power of `2`, we can replace that with a left bit shift. In fact, it doesn't have to be a power of two if we have a multiplication by some other number, that can be replaced by some combination of shifting and subtractions. I want to point out that **these transformations will not result in any kind of speedup** because, on modern machines, multiplication is just as fast as any other single instruction. 
 
### constant folding
Operations on constants can be computed at compile time. 
```C
x := 2 + 2;  => x := 4;
if  2 > 0 jump L  => jump L;
```

There is one situation that you should be aware of in which constant folding can be very dangerous. It's something that really illustrates some of the subtleties of program optimization and programming language semantics. Let's consider the scenario where we have two machines, X, Y.   Now the compiler is running on machine X, and the compiler is producing code for machine Y, and the code will run on it. This is called cross-compiler(Just considering the embedded system code). The problem comes, if X and Y are different architectures. So let's say we have the instruction `a := 1.5 + 3.7;`, and you would like to constant fold that down to equal `5.2`. Now the problem is that if you simply execute this as a floating-point operation on X, the roundoff and the pointing number semantics may be slightly different from Y. So on Y you may get something like `a:= 5.19;` . There might be a small difference in the floating-point result. 


### Eliminate unreachable basic blocks
An unreachable basic block is one that is not the target of any jump or falls through. It will make the code smaller and run faster because of the cache effects to increase the spatial locality.

#### Why would unreachable basic blocks occur?
There are several ways in which unreachable code can arise.  The most common cause is that the code is actually parameterized with code that is only compiled and used in certain situations. In c, it would be sort of typical to see some code that looks like this:

```C
#define DEBUG 0

if(DEBUG){ ...}
```

Another case where unreachable code comes up is with libraries. So very frequently,  programs are written to use generic libraries that the program might only use a very small part of the interface. But the library might supply hundreds of methods to cover all the situations. But for your program, you might only be using three of those methods and the rest of the methods could potentially be removed from the final binary to make the code smaller. 

And finally,  another way that unreachable basic blocks occur is as the result of other optimizations. So as we can see, optimizations frequently lead to more optimizations. And it could just be through other rearrangements of the compiler to make some redundant BBs removed. 

### Some optimizations are simplified if each register occurs only once on the left-hand side of an assignment.

If each register is assigned at most once then some of these optimizations are easier to talk about.

#### Common subexpression elimination

In SSA and `x := ` is the first use of `x` in a block. Then when two assignments have the same rhs, they compute the same value.

```C
x := y + z;
...
w := y + z;
```
can be replaced by :

```C
x := y + z;
...
w := x;
```

#### copy propogation

If `w := x `appears in a block, replace subsequent uses of `w` with the use of `x`.

```C
b := z + y;
a := b;
x := 2 * a;
```

can be replaced by:

```C
b := z + y;
a := b;
x := 2 * b;
```

This optimization can only be useful in conjunction with some of other optimizations such as constant folding and dead code elimination.

### Sumarry

Each of these optimizations presented actually doesn't make the program run faster at all. They don't make program run slower either, but by themselves, they don't actually make any improvement to the program. But typically, the optimizations will interact , so performing one optimization will enable another. So the way to think about an optimizing compiler is that it has a big bag of tricks(program transformations), when faced with a program to optimize,  it is going to rummage around in its bag looking for an optimization that applies to some part of the code. If it finds one,  it will do the optimization. And it will repeat it and look back and see if there is another optimization that applies. Then it will just keep doing this until it reaches a point where none of the optimizations it knows can be applied. 

### 14-04 Peephole Optimizations

Let's see a variation on local optimization that applies directly to assembly code called peephole optimization. Peephole optimization in one such technique that peephole stands for a short (usually continuous) sequence of instructions, what optimizer will do is to replace the sequence with another equivalent one(but faster). 

One example is:

```asm
move $a $b
move $b $a
```

can be replaced by the following code if `move $b $a` is not the target of a jump.

```asm
move $a $b
```

Another example is

```asm
addiu $a $a i;
addiu $a $a j;
```
can be replaced by the following code

```asm
addiu $a $a i+j;
```

Many basic block optimizations can be cast also as peephole optimizations.:

* `addiu $a $b 0` -> `move $a $b`
* `move $a $a` -> 

Peephole optimizations must be applied repeatedly for maximum effect. In fact, program optimization is grossly misnamed, I think program improvement is a more appropriate term because compilers will just improve the program as much as they can, and there is no guarantee that the best code will be produced. 



### 18-01 Register Allocation
#### Overview
Intermediate code can use an unlimited number of temporaries, this simplifies optimization because they don't have to worry about preserving the right number of registers in the code but it does complicate the final translation into assembly code because we might be using too many temporaries and this is actually a problem in practice. So it's common for intermediate code to use more temporaries than physical registers on the machine. Then the problem is to rewrite the intermediate code to use no more temporaries than physical registers. And we are going to assign multiple temporaries to each register. So we are going to have a many-one mapping from temporaries to registers. How can we actually make a single register hold multiple values? Well, the trick is that it's fine for registers to have local values as long as it only has one value at a time.

Let's consider this program:

```C
a := c + d;
e := a + b;
f := e - 1;
```

Here, I'm assuming that a and e are not used anywhere else and so it turns out that a, e, and f could all actually live in the same register. Alright, that's assuming that a and e are dead after their uses. And what will that look like, well let's allocate them all to a particular register `r1` and let's assign `c`, `d`, and `b` into their own individual registers and the code would like this:

```C
r1 := r2 + r3;
r1 := r1 + r4;
r1 := r1 - 1;
```

And so now notice this is just a translation of the code over here into registers but there is a many one mapping of names above to register names below.

#### History
A register allocation is an old problem. In fact, it was first recognized way back in the 1950s in the original Fortran project but originally, register allocation was done with a fairly crude algorithm, and someone rapidly or very quickly noticed that was actually a bottleneck in the quality of code generation that actually limitations on the ability of register allocation have a really significant effect on the overall quality of the code that compilers could produce. And then about 30 years later, in 1980, a breakthrough occurred where a group of researchers at IBM discovered a register allocation scheme based on graph coloring. And the great thing about this scheme is that it's pretty simple. It's easy to explain. It's global, meaning it takes advantage of
information from the entire control flow graph at the same time and also happens to work well in practice. 

#### register interface graph(RIG)

And here's the basic principle that underlies the modern register allocation algorithms. So, if I have two temporaries `t1` and `t2`, I want to know when they can share register. So, **they're allowed to share a register if they are not live at the same time.**  

Let's take a look at a control flow graph and now, we know that in order to do the register allocation to solve the register allocation at least in this in this way, we're going to need liveness information. Well, we're going to construct an undirected graph and in this graph, there will be a node for each temporary so each variable will have a node in the graph and there'll be an edge between two temporaries if they are live simultaneously at some point in the program. This is the graph (RIG) constructed from the code and the line analysis above, it's easy to read off from the graph what the constraints are. 

![image](https://user-images.githubusercontent.com/45984215/159105388-a2ae4110-5068-412e-9b69-c5dfff04666b.png)
![image](https://user-images.githubusercontent.com/45984215/159105394-6df657f4-0742-4d4a-9ebb-12cca0f9ad9a.png)


So, for example, b and c cannot be in the same register because b and c are connected by an edge, which means they're live simultaneously at some part, some point in the program and so they have to live in different registers. On the other hand, there is at, there is no edge between b and d. So, this edge is missing, and therefore, it's possible that b and d could be allocated in the same register. 

So a great thing about the register interference graph (RIG) is that it extracts exactly the information needed to characterize a legal register assignment. So, it gives us a representation of all the possible legal register assignments. Now, we haven't actually gotten a register assignment out of the register interference graph, but the first step is to characterize the problem in some kind of precise way. The other thing that is good about is a global view of the register requirements meaning it's over
the entire control flow graphs. So, takes into account information from every part of the control flow graph which will help us to make good global decisions about what value is very important to live in registers. And finally, the other thing to notice is that, after reconstruction, the register allocation for the algorithm is architecture-independent. 

### 18-02 Graph Coloring

A graph coloring is an assignment of colors to nodes such that the nodes connected by an edge have different colors. And then the graph is k-colorable if it has a coloring that uses k or fewer colors. In our problem, the colors correspond to registers so we want to do is to assign colors or registers to the graph nodes. And we're going to let k, the number, the maximum number of colors we're allowed to use be the number of machine registers. 

![image](https://user-images.githubusercontent.com/45984215/159107869-5f61d491-4723-4fc5-ab83-45779a4ab097.png)
![image](https://user-images.githubusercontent.com/45984215/159107891-b065bbdf-0ac9-4d0f-aa80-c281cff6fe9d.png)
![image](https://user-images.githubusercontent.com/45984215/159107900-eca3de7c-a552-4936-88b2-42c08e84e141.png)


#### Algorithm
* Step1
   *  Pick a node `t` with fewer than k neighbors in RIG
   * Eliminate t and its edges from RIG
   * If the resulting graph is k-colorable, then so is the original graph
* Step2: Assign colors to nodes on the stack 
   * Start with the last node added
   * At each step pick a color different from those assigned to already colored neighbors


### 16-03 Spilling

In this lecture, we are going to continue our discussion of register allocation, and this time we are going to talk about what happens when we can't successfully color the graph, in which case we have to do something known as spilling. The graph coloring algorithm that we discussed in the previous last doesn't always succeed in coloring an arbitrary graph. And it may well get stuck and not be able to find a coloring. And so in that case the only conclusion we can reach is that we can't hold all the values to register. And those temporary values have to live somewhere so where should they live? Well, they're going to have to live in memory. That's the only other kind of story that we have. And so we're going to pick some values and spill them into memory. The ideas that we have, the
picture in your mind should be a bucket and it can hold a fixed amount of stuff, those are the registers and when it gets too full, some of the stuff spills over, and ends up someplace else. **Now, when does the graph coloring do get stuck?** Well, the only situation in which we won't be able to make progress is all the nodes have k or more neighbors. Given that the machine we want to use only has three registers and so we, instead of finding a 4-coloring of this graph, we need to find a 3-coloring. So let's think about how to find the three coloring of this graph. If we apply the heuristic, we'll remove A from the graph but then we're going to get stuck. Because once you take `A` out of the graph and its edge is out and every node left has three or more neighbors as at least three neighbors. So, there's no node that we can delete from the graph and be guaranteed to be able to find the coloring for it with the heuristic that we discussed in the previous lecture. So, in this situation, what we're going to do is we're going to pick and know that there is a candidate for spilling. This is a node that we think we may have to assign into a memory location rather than to our register.

![image](https://user-images.githubusercontent.com/45984215/159150863-b9e1bda2-e8f5-422d-ace5-4a6687399d29.png)

![image](https://user-images.githubusercontent.com/45984215/159150867-a96d109e-828f-4e39-8e1a-9b35eb8f627f.png)


Let's assume for the sake of this example that we pick `f` and we're going to spill `f`.  Now we have to try to assign a color to `f` and it could be, we could get lucky and discover that even though `f` had more than there neighbors or three or more neighbors when we remove it from the graph, it could be that when we go to construct the coloring for the subgraph that those neighbors actually don't use all of the registers. And so, this is called **optimistic coloring**. So we pick a candidate for spilling. We tried to color the subgraph. Once we have a coloring for the sub-graph, now we see if we just get lucky are able to assign a register to `f` in which case we can just go ahead and continue the color of the rest of the graph as if nothing had
happened. 

So in this case let's take a look at what happens. We're going to add `f` back into the graph.  And in this case, optimistic coloring will not work so, in fact, `f` had more than K neighbors and after we color the sub-graph, it turns out that those neighbors are using all K. And so there is no register leftover for `f` and we're going to have to
actually spill it and store it in memory. So, if optimistic coloring fails as it does in
this example, then we spill `f`. So, what we're going to do is allocate the memory
location for `f` and typically, what that means is that we'll allocate a position in
the current stack frame. Let's call this address `fa` for the address of  `f`. And then
we're going to modify the control flow graph. We're going to change the code for
that compiling. So, before each operation that reads `f`, we're going to insert a load
that loads from that address to the current value of  `f` into a temporary name. And similarly, after each operation that writes `f`, we're going to insert the store so we're going to save the current value of `f` into its location in memory. So, here is the original code from which we constructed the registry interference graph and notice that there are few references to f in here and we just highlight them, alright. 

![image](https://user-images.githubusercontent.com/45984215/159151093-205118ec-0a26-460a-9313-bbd163597693.png)

![image](https://user-images.githubusercontent.com/45984215/159151101-bd6d86a1-c2a2-4654-bb0f-9179864f03be.png)


So, here we had the use of `f`, the read of `f` in this statement and now we preceded that by a load. And notice that I've given a new name here I've called `f1`, that's because the different uses of `f` in the control flow graph don't all have to have the same temporary name and actually, it would be a good idea to separate them so
each distinct to use of `f` will get its own name. So here we load the value of `f`
and then it gets to use in the statement. Here we have a write to `f` and so we store
the current value of `f` to a different name `f2`. And finally, the third use of `f` there's another load of `f` right here which is then used in this computation here of `b`. Okay. So, that is the systematic way to modify the code to use `f` in storage. And now, we have to recompute the aliveness of `f`.  And so, whathappens there? Well, here is the original aliveness information from which we computed the register interference graph, okay.
![image](https://user-images.githubusercontent.com/45984215/159151281-7daa19bc-e182-4fe6-914c-59c3834d1109.png)


And now notice that `f` is gone. We no longer use `f` in the programs so we can delete all the places where we mentioned that `f` was live and now we have the three new names, `f1`, `f2`, and `f3`. And we have to add in their aliveness information so it creates a new program where we inserted statements. And of course, where we have a load of the current value of `f` lives right before the use in the next statement. Here, we have the right of the current value of `f` and that's live right before the store, and then here's another load of the current value of `f` which is live until the use in the next statement. Okay. And so, now notice here that `f` used to be live in many places in the code. And now not only is `f` of the different versions in fewer places also we've distinguished them. So, it actually separates the different uses of `f` and so this will
have their own nodes in their own set of interferences in the graph and they won't
share them with the other users of `f` and that will actually also reduce the number
of edges in the graph. 


To summarize the example above, once we have decided that we are actually going to spill a temporary `f`, that means we're going to change the program where have loads and stores to the program and now we're going to have a different program and that's going to change our register allocation problems. We're going to have to recompute the aliveness of information, we have to rebuild the restrain interference graph and then we're going to have to try again to color that block graph. Now, it turns out that this new aliveness information is almost the same as it was before. So, all the temporary names other than `f` are not much affected by new statements that are added. And, `f` itself has changed fairly dramatically. Certainly the old name `f` is no longer used and so it's like this information goes away and then we've also split `f` into three, in this case, three different temporaries, one for each of the different uses of `f` in the control flow graph. And I noticed that each of these new uses of `f` or these new versions of `f` is live in a very, very small area. 

For a load instruction, `f` is live only between the load and the next instruction where it's used and similarly for a store, `f` is live only between the store itself and the proceeding instruction, the one they created `f`.  **And the effect  is to greatly reduce the live range of the spilled variable**. So because the live range of `f` is reduced by
spilling, it has fewer interferences in the new program than it did in the old program. 

And so what that means the particulars in the rebuild register interference graph. `f` will have fewer neighbors. Some of the neighbors that it had before have gone away because it's live in fewer places. 

![image](https://user-images.githubusercontent.com/45984215/159151824-ae19cdb0-dcad-49fe-8caa-5bce47e7b917.png)

So if we look at the new register interference graph, we can see that among all the different versions of `f`. Remember that `f` has been split into three temporaries in this graph. We see that they only interfere with `d` and `c`, whereas before f have several other neighbors in the graph. And now, in fact, this new graph is three colorable. 


Of course, it might be the case that we can't just spill one name. We might have to spill several different temporaries before the coloring is found. And, the tricky part is what to spill. So, this is the hard decision that has to be made during restore allocation. Now any choice is correct. It's only a question of performance so you know some choices of spilling will lead to better code than others but any choice of spilling is going to resolve in a correct program. And there's heuristics that people use to pick which temporaries to spill and here are a few or I think three of the most popular ones. One is to spill the temporaries have the most conflicts. And the reason for that is that the temporary will most affect the number of interferences in the graph. We'll remove enough edges from the graph that they become tolerable with the number of registers we have. Another possibility is spilling temporaries that have few definitions and uses. And, here the idea is that by spilling those since they're not used very much, the number of load and write in storage will have to add, will be relatively small and so if a variable just isn't used in many places, then the actual cost, in terms of additional instructions that are going to be executed to spill it, is relatively small. And another one and this is actually the one that I think that all the compilers implement is to avoid spilling in an inner loops. So, if you have a choice between spilling a variable that's used within innermost loop for the program and one that is used someplace else. It's probably preferred that you spill the one that is used not in the innermost loop absolutely because again, that will result in fewer loads in stores. You really want to avoid adding additional instructions to your inner loop.
