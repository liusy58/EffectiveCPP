## 14-01 Intermediate language
#### What is Intermediate language?
The intermediate language is a language between the source and the target.

#### Why bother to introduce Intermediate language?
Because it provides an intermediate level of abstraction, it has more details than the source. For example, we want to optimize register usage, most source languages have no notion of the register at the source level so there is no way to even express the kinds of optimization you might want to do with registers.
It will also have fewer details than the target. For example, the intermediate language is a little bit above the level of the particular instruction set of a particular machine, and therefore, it is easier to retarget that intermediate level of code to lots of different kinds of machines because it doesn't have all the grubby details of a particular machine.

## 14-02 Optimization Overview

#### When should we perform optimizations?

In fact, we can perform them on the AST, a big advantage of that is that it's machine-independent, but it turns out AST is too high for a lot of optimizations we want to do because optimizations depend on lower-level details of the machine. Another possibility would be to perform optimizations directly on the assembly language but they are machine-dependent and we would have to reimplement optimizations for each kind of architecture. So intermediate language is an ideal choice.

#### What is the purpose of optimization?
The purpose of optimization is to improve a program's resource utilization such as execution time, code size, network messages sent, and so on.
For languages like C and Cool,  there are three granularities of optimizations. One is called Local optimizations which occur within a single block. Then there're what are called Global optimizations. This is really misnamed because it is not global across the entire program, it's global across an entire function. So global optimizations would apply to a single function and apply to all the basic blocks of that function. And finally, there are inter-procedural optimizations. These are optimizations that work across method boundaries. They take multiple functions and move things around to try to optimize the collection of functions as a whole. 

#### In practice, often a conscious decision is made not to implement the fanciest optimization known, why?
First, some optimizations are hard to implement for SE. 
Second, some optimizations are costly in compilation time. Even though the compiling happens offline not part of the running of the program, the programmer still has to wait when the optimizing compiler does its optimizations. 
Third, some of these optimizations have a low payoff. They might only do it by a small amount.
Last, unfortunately, many fancy optimizations are all there!

So the goal of optimization is to maximum benefit for minimum cost.

## 14-03 Local Optimizations

### Algebraic Simplification

#### Some statements can  be deleted.
```C
x := x + 0; 
x := x * 1;
```
#### Some statements can be simplified.
```C
x := x * 0;     =>  x := 0;
y := y ** 2;    =>  y := y * y;
z := z * 8;     =>  z := z << 3;
p := p * 15;    =>  t := p << 4; p : = t - p;
```

For the operator `**`, it's probably that's going to wind up in our generated code is a call to some built-in math library which will introduce the function call overhead and some kind of general loop in there. So in a special case where we know the exponent is 2, it's much more efficient to just replace that call to exponentiate by an explicit multiply. 

Another example is `z := z * 8;`. If we have a multiplication by a power of `2`, we can replace that with a left bit shift. In fact, it doesn't have to be a power of two if we have a multiplication by some other number, that can be replaced by some combination of shifting and subtractions. I want to point out that **these transformations will not result in any kind of speedup** because, on modern machines, multiplication is just as fast as any other single instruction. 
 
### constant folding
Operations on constants can be computed at compile time. 
```C
x := 2 + 2;  => x := 4;
if  2 > 0 jump L  => jump L;
```

There is one situation that you should be aware of in which constant folding can be very dangerous. It's something that really illustrates some of the subtleties of program optimization and programming language semantics. Let's consider the scenario where we have two machines, X, Y.   Now the compiler is running on machine X, and the compiler is producing code for machine Y, and the code will run on it. This is called cross-compiler(Just considering the embedded system code). The problem comes, if X and Y are different architectures. So let's say we have the instruction `a := 1.5 + 3.7;`, and you would like to constant fold that down to equal `5.2`. Now the problem is that if you simply execute this as a floating-point operation on X, the roundoff and the pointing number semantics may be slightly different from Y. So on Y you may get something like `a:= 5.19;` . There might be a small difference in the floating-point result. 


### Eliminate unreachable basic blocks
An unreachable basic block is one that is not the target of any jump or falls through. It will make the code smaller and run faster because of the cache effects to increase the spatial locality.

#### Why would unreachable basic blocks occur?
There are several ways in which unreachable code can arise.  The most common cause is that the code is actually parameterized with code that is only compiled and used in certain situations. In c, it would be sort of typical to see some code that looks like this:

```C
#define DEBUG 0

if(DEBUG){ ...}
```

Another case where unreachable code comes up is with libraries. So very frequently,  programs are written to use generic libraries that the program might only use a very small part of the interface. But the library might supply hundreds of methods to cover all the situations. But for your program, you might only be using three of those methods and the rest of the methods could potentially be removed from the final binary to make the code smaller. 

And finally,  another way that unreachable basic blocks occur is as the result of other optimizations. So as we can see, optimizations frequently lead to more optimizations. And it could just be through other rearrangements of the compiler to make some redundant BBs removed. 

### Some optimizations are simplified if each register occurs only once on the left-hand side of an assignment.

If each register is assigned at most once then some of these optimizations are easier to talk about.

#### Common subexpression elimination

In SSA and `x := ` is the first use of `x` in a block. Then when two assignments have the same rhs, they compute the same value.

```C
x := y + z;
...
w := y + z;
```
can be replaced by :

```C
x := y + z;
...
w := x;
```

#### copy propogation

If `w := x `appears in a block, replace subsequent uses of `w` with the use of `x`.

```C
b := z + y;
a := b;
x := 2 * a;
```

can be replaced by:

```C
b := z + y;
a := b;
x := 2 * b;
```

This optimization can only be useful in conjunction with some of other optimizations such as constant folding and dead code elimination.

### Sumarry

Each of these optimizations presented actually doesn't make the program run faster at all. They don't make program run slower either, but by themselves, they don't actually make any improvement to the program. But typically, the optimizations will interact , so performing one optimization will enable another. So the way to think about an optimizing compiler is that it has a big bag of tricks(program transformations), when faced with a program to optimize,  it is going to rummage around in its bag looking for an optimization that applies to some part of the code. If it finds one,  it will do the optimization. And it will repeat it and look back and see if there is another optimization that applies. Then it will just keep doing this until it reaches a point where none of the optimizations it knows can be applied. 

### 14-04 Peephole Optimizations

Let's see a variation on local optimization that applies directly to assembly code called peephole optimization. Peephole optimization in one such technique that peephole stands for a short (usually continuous) sequence of instructions, what optimizer will do is to replace the sequence with another equivalent one(but faster). 

One example is:

```asm
move $a $b
move $b $a
```

can be replaced by the following code if `move $b $a` is not the target of a jump.

```asm
move $a $b
```

Another example is

```asm
addiu $a $a i;
addiu $a $a j;
```
can be replaced by the following code

```asm
addiu $a $a i+j;
```

Many basic block optimizations can be cast also as peephole optimizations.:

* `addiu $a $b 0` -> `move $a $b`
* `move $a $a` -> 

Peephole optimizations must be applied repeatedly for maximum effect. In fact, program optimization is grossly misnamed, I think program improvement is a more appropriate term because compilers will just improve the program as much as they can, and there is no guarantee that the best code will be produced. 



### 18-01 Register Allocation
#### Overview
Intermediate code can use an unlimited number of temporaries, this simplifies optimization because they don't have to worry about preserving the right number of registers in the code but it does complicate the final translation into assembly code because we might be using too many temporaries and this is actually a problem in practice. So it's common for intermediate code to use more temporaries than physical registers on the machine. Then the problem is to rewrite the intermediate code to use no more temporaries than physical registers. And we are going to assign multiple temporaries to each register. So we are going to have a many-one mapping from temporaries to registers. How can we actually make a single register hold multiple values? Well, the trick is that it's fine for registers to have local values as long as it only has one value at a time.

Let's consider this program:

```C
a := c + d;
e := a + b;
f := e - 1;
```

Here, I'm assuming that a and e are not used anywhere else and so it turns out that a, e, and f could all actually live in the same register. Alright, that's assuming that a and e are dead after their uses. And what will that look like, well let's allocate them all to a particular register `r1` and let's assign `c`, `d`, and `b` into their own individual registers and the code would like this:

```C
r1 := r2 + r3;
r1 := r1 + r4;
r1 := r1 - 1;
```

And so now notice this is just a translation of the code over here into registers but there is a many one mapping of names above to register names below.

#### History
A register allocation is an old problem. In fact, it was first recognized way back in the 1950s in the original Fortran project but originally, register allocation was done with a fairly crude algorithm, and someone rapidly or very quickly noticed that was actually a bottleneck in the quality of code generation that actually limitations on the ability of register allocation have a really significant effect on the overall quality of the code that compilers could produce. And then about 30 years later, in 1980, a breakthrough occurred where a group of researchers at IBM discovered a register allocation scheme based on graph coloring. And the great thing about this scheme is that it's pretty simple. It's easy to explain. It's global, meaning it takes advantage of
information from the entire control flow graph at the same time and also happens to work well in practice. 

#### register interface graph(RIG)

And here's the basic principle that underlies the modern register allocation algorithms. So, if I have two temporaries `t1` and `t2`, I want to know when they can share register. So, **they're allowed to share a register if they are not live at the same time.**  

Let's take a look at a control flow graph and now, we know that in order to do the register allocation to solve the register allocation at least in this in this way, we're going to need liveness information. Well, we're going to construct an undirected graph and in this graph, there will be a node for each temporary so each variable will have a node in the graph and there'll be an edge between two temporaries if they are live simultaneously at some point in the program. This is the graph (RIG) constructed from the code and the line analysis above, it's easy to read off from the graph what the constraints are. 

![image](https://user-images.githubusercontent.com/45984215/159105388-a2ae4110-5068-412e-9b69-c5dfff04666b.png)
![image](https://user-images.githubusercontent.com/45984215/159105394-6df657f4-0742-4d4a-9ebb-12cca0f9ad9a.png)


So, for example, b and c cannot be in the same register because b and c are connected by an edge, which means they're live simultaneously at some part, some point in the program and so they have to live in different registers. On the other hand, there is at, there is no edge between b and d. So, this edge is missing, and therefore, it's possible that b and d could be allocated in the same register. 

So a great thing about the register interference graph (RIG) is that it extracts exactly the information needed to characterize a legal register assignment. So, it gives us a representation of all the possible legal register assignments. Now, we haven't actually gotten a register assignment out of the register interference graph, but the first step is to characterize the problem in some kind of precise way. The other thing that is good about is a global view of the register requirements meaning it's over
the entire control flow graphs. So, takes into account information from every part of the control flow graph which will help us to make good global decisions about what value is very important to live in registers. And finally, the other thing to notice is that, after reconstruction, the register allocation for the algorithm is architecture-independent. 

### 18-02 Graph Coloring

A graph coloring is an assignment of colors to nodes such that the nodes connected by an edge have different colors. And then the graph is k-colorable if it has a coloring that uses k or fewer colors. In our problem, the colors correspond to registers so we want to do is to assign colors or registers to the graph nodes. And we're going to let k, the number, the maximum number of colors we're allowed to use be the number of machine registers. 

![image](https://user-images.githubusercontent.com/45984215/159107869-5f61d491-4723-4fc5-ab83-45779a4ab097.png)
![image](https://user-images.githubusercontent.com/45984215/159107891-b065bbdf-0ac9-4d0f-aa80-c281cff6fe9d.png)
![image](https://user-images.githubusercontent.com/45984215/159107900-eca3de7c-a552-4936-88b2-42c08e84e141.png)


#### Algorithm
* Step1
   *  Pick a node `t` with fewer than k neighbors in RIG
   * Eliminate t and its edges from RIG
   * If the resulting graph is k-colorable, then so is the original graph
* Step2: Assign colors to nodes on the stack 
   * Start with the last node added
   * At each step pick a color different from those assigned to already colored neighbors
